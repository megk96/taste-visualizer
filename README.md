# Taste Visualizer
This is an ongoing expedition to visualize a user's taste using both deterministic visualization tools and generative apporaches to develop creative representations to discern and understand taste. 

## Executive Summary
This expedition explores a few inference diffusion models to explore representations of a user's taste. 
* **Conceptualization**: The first part of this expedition is to generate the text (information) that we would want to then visually represent. 
* **Visualization**: The second part of this expedition is to use the output from conceptualization and try out different generative diffusion models and hyperparameters for this use-case. 


## Conceptualization
* **Iteration 1**: Use movie/book/TV show titles to randomly sample taste

* **Iteration 2**: Use "vibes" generated by a clustering algorithm and named by GPT-4 API to name the different "sections" of a user's taste. 

* **Iteration 3**: Use randomly sampled keywords generated in an upstream expedition to represent a user's taste. 

## Visualization
* **Stable Diffusion**: The stable diffusion inference is adapted from [this code](https://colab.research.google.com/drive/1roZqqhsdpCXZr8kgV_Bx_ABVBPgea3lX?usp=sharing) and experimented with parameters and prompting from the most promising iterations from Conceptualization stage. This generates an image for each of the "vibes" generated (Iteration 2 in Conceptualization) and is then stitched into a video. 

* **Stable Diffusion for Videos**: This generates morphed videos using a Stable Diffusion Walk Pipeline adopted from [this code](https://github.com/nateraw/stable-diffusion-videos). 

    The user's taste is represented as 5 vibes and an animation is generated as the video morphs from Vibe 1 to Vibe 2 all the way to Vibe 5. The experimentation plays with the hyperparameters and prompting to get a more seamless morphing.

    It is observed that abstract concepts like "gritty stories" or "psychological thrillers" don't perform so well. The prompting works best for texts that are of the form "visually descriptive adjective (color/shape/form) + common noun". Most of the vibes generated from the upstream task are quite abstract and nuanced. We would need to revisit conceptualization to attempt to transform it to the form mentioned above. 

* **Stable Diffusion with Music**: An experiment to add music to the video is trialled. This needs more work on selecting audio to match the vibes as it progresses. It is simply tried as a fun experiment, for now.

## Gaps to Bridge
* There is random text generation with some of these prompts. The prompts are sometimes movie-related-phrases like "chilling masterpieces" so it attempts to generate a poster but the words end up being gibberish. 
* The morphing didn't seem entirely successful when the vibes are not necessarily arranged ordinally. An ordinal representation of the vibes based on how they are geometrically placed in the embedding space from which they are generated could be a good approach to order them for a more seamless transition.
* Alternatives to slow morph videos can be considered - like generating clip art of the different vibes of a user's taste and all superimposed onto a single canvas. This would require using datasets that have this style to condition the model to generate in a particular style. 
